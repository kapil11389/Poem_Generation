{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: IMPORTING DEPENDENCIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: IMPORTING THE DATA\n",
    "text = open(\"./sonnets.txt\").read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3 : CREATE A CHARACTER NUMBER MAPPING\n",
    "characters = sorted(list(set(text)))\n",
    "n_to_char = { n:char for n,char in enumerate(characters)}\n",
    "char_to_n = { char:n for n,char in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4 : DATA PRE-PROCESSING\n",
    "# Defining what is Training data and what is the correct label according to that?\n",
    "# In this case, if the word is mannequin.\n",
    "# Labels would be as follows if the sequence length we consider for generation is 5:\n",
    "# m, a, n, n, e -> q\n",
    "# a, n, n, e ,q -> u\n",
    "# n, n, e, q, u -> i\n",
    "# and so on\n",
    "\n",
    "X = [] # Training data\n",
    "Y = [] # Labels corresponding to Training data\n",
    "\n",
    "length = len(text)\n",
    "seq_length = 100\n",
    "\n",
    "for i in range(length - seq_length):\n",
    "    cur_seq = text[i:i+seq_length]\n",
    "    cur_label = text[i+seq_length]\n",
    "    \n",
    "    X.append([char_to_n[char] for char in cur_seq ])\n",
    "    Y.append(char_to_n[cur_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: DATA PRE-PROCESSING \n",
    "# Now we have to scale our data such that algorithm doesnt get stuck\n",
    "# LSTM layer accepts inputs in a different manner\n",
    "# X -> numberOfSequences,LengthOfEachSequence,NumberOfFeatures\n",
    "# Y -> One hot encoding should be present\n",
    "\n",
    "X_modified = np.reshape(X,(len(X),seq_length,1))\n",
    "X_modified = X_modified/float(len(characters))\n",
    "Y_modified = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: MODELLING\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer contains 400 memory units\n",
    "LSTM_layer_1 = LSTM(400,input_shape=(X_modified.shape[1],X_modified.shape[2]),return_sequences=True)\n",
    "model.add(LSTM_layer_1)\n",
    "\n",
    "# First Dropout layer\n",
    "Dropout_layer_1 = Dropout(0.2)\n",
    "model.add(Dropout_layer_1)\n",
    "\n",
    "# Second LSTM layer contains another 400 memory units\n",
    "LSTM_layer_2 = LSTM(400,return_sequences=True)\n",
    "model.add(LSTM_layer_2)\n",
    "\n",
    "# Second Dropout layer\n",
    "Dropout_layer_2 = Dropout(0.2)\n",
    "model.add(Dropout_layer_2)\n",
    "\n",
    "model.add(LSTM(400))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Dense layer for results\n",
    "Dense_layer_1 = Dense(Y_modified.shape[1],activation='softmax')\n",
    "model.add(Dense_layer_1)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running the model created\n",
    "model.fit(X_modified,Y_modified,epochs=100,batch_size=100)\n",
    "models.save_weights('./weights.h5')\n",
    "model.load_weights('./weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6 : GENERATING TEXT\n",
    "string_mapped  = X[100] # Selecting a random line of text (values corresponding to text)\n",
    "full_string = [n_to_char[value] for value in string_mapped] # The generated text character by character\n",
    "# generating characters\n",
    "for i in range(400):\n",
    "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "    x = x / float(len(characters))\n",
    "\n",
    "    pred_index = np.argmax(model.predict(x, verbose=0)) # Predicted character\n",
    "    seq = [n_to_char[value] for value in string_mapped]\n",
    "    full_string.append(n_to_char[pred_index])\n",
    "\n",
    "    string_mapped.append(pred_index)\n",
    "    string_mapped = string_mapped[1:len(string_mapped)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to time ou love sast dieeess,\n",
      " or mature's beauty were beauty's srete to thee,\n",
      " and therefore for my sinfuless and mose seem\n",
      " tines have proros darhon the show appear;\n",
      " and therefore for my self thy sweet self grows'st\n",
      " the linds of shat which thou deservest mew;\n",
      " shou danst not then i say mot tell me boing?\n",
      " and therefore from their thotue ont so much  and steetly griends have done thee soreng:\n",
      " whth\n"
     ]
    }
   ],
   "source": [
    "# STEP 7 : PRINTING THE OUTPUT\n",
    "txt=\"\"\n",
    "for char in full_string:\n",
    "    txt = txt+char\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Further Improvements:</strong></p>\n",
    "<p><span style=\"font-size: 10pt;\">More processing can be used to print out even more meaningful sentences. Here, we can see that machine has understood the concept of rhymes and and generates word with proper spellings most of the time.<br /></span></p>\n",
    "<p><span style=\"font-size: 10pt;\">If more time can be devoted to the processing of data, even better results can be acheived.</span></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
